{"is_success": "Fail", "type": "extract_local_code", "params": "2022-10-31", "err_msg": "cannot resolve '`병원명`' given input columns: [HOS_ID, X좌표값, Y좌표값, 경도, 구조사수, 병상수, 사업장명, 소재지도로명주소, 소재지면적정보, 소재지시설전화번호, 소재지우편번호, 소재지지번주소, 시군명, 시군코드, 업태구분명정보, 영업상태구분코드, 영업상태명, 완화의료담당부서명, 완화의료지정형태, 위도, 의료기관종별명, 의료인수, 인허가일자, 인허가취소일자, 일반구급차대수, 입원실수, 재개업일자, 지정취소일자, 진료과목내용, 진료과목내용정보, 총면적, 총종업원수, 최초지정일자, 통합영업상태구분코드, 통합영업상태명, 특수구급차대수, 폐업일자, 허가병상수, 휴업시작일자, 휴업종료일자];\n'Project [cast(HOS_ID#99 as int) AS HOS_ID#151, '병원명 AS HOS_NAME#141, 통합영업상태명#26 AS STATUS#142, 업태구분명정보#35 AS HOS_TYPE#143, 진료과목내용정보#44 AS MEDI_COURSE#144, 소재지도로명주소#55 AS ADDR#145, 소재지우편번호#56 AS POST_CODE#146, cast(경도#57 as float) AS LONGITUDE#152, cast(위도#58 as float) AS LATITUDE#153, 소재지시설전화번호#32 AS TEL#149, 시군코드#20 AS SIDO_ID#150]\n+- Project [시군코드#20, 시군명#21, 인허가일자#22, 인허가취소일자#23, 영업상태구분코드#24, 통합영업상태구분코드#25, 통합영업상태명#26, 영업상태명#27, 폐업일자#28, 휴업시작일자#29, 휴업종료일자#30, 재개업일자#31, 소재지시설전화번호#32, 소재지면적정보#33, 사업장명#34, 업태구분명정보#35, X좌표값#36, Y좌표값#37, 의료기관종별명#38, 의료인수#39, 입원실수#40, 병상수#41, 총면적#42, 진료과목내용#43, ... 16 more fields]\n   +- Project [시군코드#20, 시군명#21, 인허가일자#22, 인허가취소일자#23, 영업상태구분코드#24, 통합영업상태구분코드#25, 통합영업상태명#26, 영업상태명#27, 폐업일자#28, 휴업시작일자#29, 휴업종료일자#30, 재개업일자#31, 소재지시설전화번호#32, 소재지면적정보#33, 사업장명#34, 업태구분명정보#35, X좌표값#36, Y좌표값#37, 의료기관종별명#38, 의료인수#39, 입원실수#40, 병상수#41, 총면적#42, 진료과목내용#43, ... 18 more fields]\n      +- Window [row_number() windowspecdefinition(_w0#100L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS HOS_ID#99], [_w0#100L ASC NULLS FIRST]\n         +- Project [시군코드#20, 시군명#21, 인허가일자#22, 인허가취소일자#23, 영업상태구분코드#24, 통합영업상태구분코드#25, 통합영업상태명#26, 영업상태명#27, 폐업일자#28, 휴업시작일자#29, 휴업종료일자#30, 재개업일자#31, 소재지시설전화번호#32, 소재지면적정보#33, 사업장명#34, 업태구분명정보#35, X좌표값#36, Y좌표값#37, 의료기관종별명#38, 의료인수#39, 입원실수#40, 병상수#41, 총면적#42, 진료과목내용#43, ... 16 more fields]\n            +- Filter (통합영업상태명#26 = 영업/정상)\n               +- Relation [시군코드#20,시군명#21,인허가일자#22,인허가취소일자#23,영업상태구분코드#24,통합영업상태구분코드#25,통합영업상태명#26,영업상태명#27,폐업일자#28,휴업시작일자#29,휴업종료일자#30,재개업일자#31,소재지시설전화번호#32,소재지면적정보#33,사업장명#34,업태구분명정보#35,X좌표값#36,Y좌표값#37,의료기관종별명#38,의료인수#39,입원실수#40,병상수#41,총면적#42,진료과목내용#43,... 15 more fields] csv\n"}
{"is_success": "Fail", "type": "extract_local_code", "params": "2022-10-31", "err_msg": "cannot resolve '`병원명`' given input columns: [HOS_ID, X좌표값, Y좌표값, 경도, 구조사수, 병상수, 사업장명, 소재지도로명주소, 소재지면적정보, 소재지시설전화번호, 소재지우편번호, 소재지지번주소, 시군명, 시군코드, 업태구분명정보, 영업상태구분코드, 영업상태명, 완화의료담당부서명, 완화의료지정형태, 위도, 의료기관종별명, 의료인수, 인허가일자, 인허가취소일자, 일반구급차대수, 입원실수, 재개업일자, 지정취소일자, 진료과목내용, 진료과목내용정보, 총면적, 총종업원수, 최초지정일자, 통합영업상태구분코드, 통합영업상태명, 특수구급차대수, 폐업일자, 허가병상수, 휴업시작일자, 휴업종료일자];\n'Project [cast(HOS_ID#99 as int) AS HOS_ID#151, '병원명 AS HOS_NAME#141, 통합영업상태명#26 AS STATUS#142, 업태구분명정보#35 AS HOS_TYPE#143, 진료과목내용정보#44 AS MEDI_COURSE#144, 소재지도로명주소#55 AS ADDR#145, 소재지우편번호#56 AS POST_CODE#146, cast(경도#57 as float) AS LONGITUDE#152, cast(위도#58 as float) AS LATITUDE#153, 소재지시설전화번호#32 AS TEL#149, 시군코드#20 AS SIDO_ID#150]\n+- Project [시군코드#20, 시군명#21, 인허가일자#22, 인허가취소일자#23, 영업상태구분코드#24, 통합영업상태구분코드#25, 통합영업상태명#26, 영업상태명#27, 폐업일자#28, 휴업시작일자#29, 휴업종료일자#30, 재개업일자#31, 소재지시설전화번호#32, 소재지면적정보#33, 사업장명#34, 업태구분명정보#35, X좌표값#36, Y좌표값#37, 의료기관종별명#38, 의료인수#39, 입원실수#40, 병상수#41, 총면적#42, 진료과목내용#43, ... 16 more fields]\n   +- Project [시군코드#20, 시군명#21, 인허가일자#22, 인허가취소일자#23, 영업상태구분코드#24, 통합영업상태구분코드#25, 통합영업상태명#26, 영업상태명#27, 폐업일자#28, 휴업시작일자#29, 휴업종료일자#30, 재개업일자#31, 소재지시설전화번호#32, 소재지면적정보#33, 사업장명#34, 업태구분명정보#35, X좌표값#36, Y좌표값#37, 의료기관종별명#38, 의료인수#39, 입원실수#40, 병상수#41, 총면적#42, 진료과목내용#43, ... 18 more fields]\n      +- Window [row_number() windowspecdefinition(_w0#100L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS HOS_ID#99], [_w0#100L ASC NULLS FIRST]\n         +- Project [시군코드#20, 시군명#21, 인허가일자#22, 인허가취소일자#23, 영업상태구분코드#24, 통합영업상태구분코드#25, 통합영업상태명#26, 영업상태명#27, 폐업일자#28, 휴업시작일자#29, 휴업종료일자#30, 재개업일자#31, 소재지시설전화번호#32, 소재지면적정보#33, 사업장명#34, 업태구분명정보#35, X좌표값#36, Y좌표값#37, 의료기관종별명#38, 의료인수#39, 입원실수#40, 병상수#41, 총면적#42, 진료과목내용#43, ... 16 more fields]\n            +- Filter (통합영업상태명#26 = 영업/정상)\n               +- Relation [시군코드#20,시군명#21,인허가일자#22,인허가취소일자#23,영업상태구분코드#24,통합영업상태구분코드#25,통합영업상태명#26,영업상태명#27,폐업일자#28,휴업시작일자#29,휴업종료일자#30,재개업일자#31,소재지시설전화번호#32,소재지면적정보#33,사업장명#34,업태구분명정보#35,X좌표값#36,Y좌표값#37,의료기관종별명#38,의료인수#39,입원실수#40,병상수#41,총면적#42,진료과목내용#43,... 15 more fields] csv\n"}
{"is_success": "Fail", "type": "extract_local_code", "params": "2022-10-31", "err_msg": "An error occurred while calling o71.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (localhost executor driver): java.sql.BatchUpdateException: ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"HOS_NAME\" (actual: 22, maximum: 20)\n\n\tat oracle.jdbc.driver.OraclePreparedStatement.generateBatchUpdateException(OraclePreparedStatement.java:10323)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10090)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeLargeBatch(OraclePreparedStatement.java:9975)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatch(OraclePreparedStatement.java:9932)\n\tat oracle.jdbc.driver.OracleStatementWrapper.executeBatch(OracleStatementWrapper.java:262)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:728)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\tSuppressed: java.sql.SQLException: ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"HOS_NAME\" (actual: 22, maximum: 20)\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:628)\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:562)\n\t\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1145)\n\t\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:726)\n\t\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:291)\n\t\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:492)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:148)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1038)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeForRowsWithTimeout(OraclePreparedStatement.java:9892)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10069)\n\t\t... 17 more\n\tCaused by: Error : 12899, Position : 153, Sql = INSERT INTO HOSPITAL_INFO (\"HOS_ID\",\"HOS_NAME\",\"STATUS\",\"HOS_TYPE\",\"MEDI_COURSE\",\"ADDR\",\"POST_CODE\",\"LONGITUDE\",\"LATITUDE\",\"TEL\",\"SIDO_ID\") VALUES (:1 ,:2 ,:3 ,:4 ,:5 ,:6 ,:7 ,:8 ,:9 ,:10 ,:11 ), OriginalSql = INSERT INTO HOSPITAL_INFO (\"HOS_ID\",\"HOS_NAME\",\"STATUS\",\"HOS_TYPE\",\"MEDI_COURSE\",\"ADDR\",\"POST_CODE\",\"LONGITUDE\",\"LATITUDE\",\"TEL\",\"SIDO_ID\") VALUES (?,?,?,?,?,?,?,?,?,?,?), Error Msg = ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"HOS_NAME\" (actual: 22, maximum: 20)\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:632)\n\t\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:893)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:69)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:745)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.sql.BatchUpdateException: ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"HOS_NAME\" (actual: 22, maximum: 20)\n\n\tat oracle.jdbc.driver.OraclePreparedStatement.generateBatchUpdateException(OraclePreparedStatement.java:10323)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10090)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeLargeBatch(OraclePreparedStatement.java:9975)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatch(OraclePreparedStatement.java:9932)\n\tat oracle.jdbc.driver.OracleStatementWrapper.executeBatch(OracleStatementWrapper.java:262)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:728)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n\tSuppressed: java.sql.SQLException: ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"HOS_NAME\" (actual: 22, maximum: 20)\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:628)\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:562)\n\t\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1145)\n\t\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:726)\n\t\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:291)\n\t\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:492)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:148)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1038)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeForRowsWithTimeout(OraclePreparedStatement.java:9892)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10069)\n\t\t... 17 more\n\tCaused by: Error : 12899, Position : 153, Sql = INSERT INTO HOSPITAL_INFO (\"HOS_ID\",\"HOS_NAME\",\"STATUS\",\"HOS_TYPE\",\"MEDI_COURSE\",\"ADDR\",\"POST_CODE\",\"LONGITUDE\",\"LATITUDE\",\"TEL\",\"SIDO_ID\") VALUES (:1 ,:2 ,:3 ,:4 ,:5 ,:6 ,:7 ,:8 ,:9 ,:10 ,:11 ), OriginalSql = INSERT INTO HOSPITAL_INFO (\"HOS_ID\",\"HOS_NAME\",\"STATUS\",\"HOS_TYPE\",\"MEDI_COURSE\",\"ADDR\",\"POST_CODE\",\"LONGITUDE\",\"LATITUDE\",\"TEL\",\"SIDO_ID\") VALUES (?,?,?,?,?,?,?,?,?,?,?), Error Msg = ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"HOS_NAME\" (actual: 22, maximum: 20)\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:632)\n\t\t... 26 more\n"}
{"is_success": "Fail", "type": "extract_local_code", "params": "2022-10-31", "err_msg": "An error occurred while calling o71.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (localhost executor driver): java.sql.BatchUpdateException: ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"MEDI_COURSE\" (actual: 112, maximum: 100)\n\n\tat oracle.jdbc.driver.OraclePreparedStatement.generateBatchUpdateException(OraclePreparedStatement.java:10323)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10090)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeLargeBatch(OraclePreparedStatement.java:9975)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatch(OraclePreparedStatement.java:9932)\n\tat oracle.jdbc.driver.OracleStatementWrapper.executeBatch(OracleStatementWrapper.java:262)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:728)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\tSuppressed: java.sql.SQLException: ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"MEDI_COURSE\" (actual: 112, maximum: 100)\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:628)\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:562)\n\t\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1145)\n\t\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:726)\n\t\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:291)\n\t\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:492)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:148)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1038)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeForRowsWithTimeout(OraclePreparedStatement.java:9892)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10069)\n\t\t... 17 more\n\tCaused by: Error : 12899, Position : 165, Sql = INSERT INTO HOSPITAL_INFO (\"HOS_ID\",\"HOS_NAME\",\"STATUS\",\"HOS_TYPE\",\"MEDI_COURSE\",\"ADDR\",\"POST_CODE\",\"LONGITUDE\",\"LATITUDE\",\"TEL\",\"SIDO_ID\") VALUES (:1 ,:2 ,:3 ,:4 ,:5 ,:6 ,:7 ,:8 ,:9 ,:10 ,:11 ), OriginalSql = INSERT INTO HOSPITAL_INFO (\"HOS_ID\",\"HOS_NAME\",\"STATUS\",\"HOS_TYPE\",\"MEDI_COURSE\",\"ADDR\",\"POST_CODE\",\"LONGITUDE\",\"LATITUDE\",\"TEL\",\"SIDO_ID\") VALUES (?,?,?,?,?,?,?,?,?,?,?), Error Msg = ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"MEDI_COURSE\" (actual: 112, maximum: 100)\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:632)\n\t\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:893)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:69)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:745)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.sql.BatchUpdateException: ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"MEDI_COURSE\" (actual: 112, maximum: 100)\n\n\tat oracle.jdbc.driver.OraclePreparedStatement.generateBatchUpdateException(OraclePreparedStatement.java:10323)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10090)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeLargeBatch(OraclePreparedStatement.java:9975)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatch(OraclePreparedStatement.java:9932)\n\tat oracle.jdbc.driver.OracleStatementWrapper.executeBatch(OracleStatementWrapper.java:262)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:728)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n\tSuppressed: java.sql.SQLException: ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"MEDI_COURSE\" (actual: 112, maximum: 100)\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:628)\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:562)\n\t\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1145)\n\t\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:726)\n\t\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:291)\n\t\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:492)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:148)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1038)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeForRowsWithTimeout(OraclePreparedStatement.java:9892)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10069)\n\t\t... 17 more\n\tCaused by: Error : 12899, Position : 165, Sql = INSERT INTO HOSPITAL_INFO (\"HOS_ID\",\"HOS_NAME\",\"STATUS\",\"HOS_TYPE\",\"MEDI_COURSE\",\"ADDR\",\"POST_CODE\",\"LONGITUDE\",\"LATITUDE\",\"TEL\",\"SIDO_ID\") VALUES (:1 ,:2 ,:3 ,:4 ,:5 ,:6 ,:7 ,:8 ,:9 ,:10 ,:11 ), OriginalSql = INSERT INTO HOSPITAL_INFO (\"HOS_ID\",\"HOS_NAME\",\"STATUS\",\"HOS_TYPE\",\"MEDI_COURSE\",\"ADDR\",\"POST_CODE\",\"LONGITUDE\",\"LATITUDE\",\"TEL\",\"SIDO_ID\") VALUES (?,?,?,?,?,?,?,?,?,?,?), Error Msg = ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"MEDI_COURSE\" (actual: 112, maximum: 100)\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:632)\n\t\t... 26 more\n"}
{"is_success": "Fail", "type": "extract_local_code", "params": "2022-10-31", "err_msg": "An error occurred while calling o71.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (localhost executor driver): java.sql.BatchUpdateException: ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"MEDI_COURSE\" (actual: 311, maximum: 200)\n\n\tat oracle.jdbc.driver.OraclePreparedStatement.generateBatchUpdateException(OraclePreparedStatement.java:10323)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10090)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeLargeBatch(OraclePreparedStatement.java:9975)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatch(OraclePreparedStatement.java:9932)\n\tat oracle.jdbc.driver.OracleStatementWrapper.executeBatch(OracleStatementWrapper.java:262)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:728)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\tSuppressed: java.sql.SQLException: ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"MEDI_COURSE\" (actual: 311, maximum: 200)\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:628)\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:562)\n\t\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1145)\n\t\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:726)\n\t\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:291)\n\t\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:492)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:148)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1038)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeForRowsWithTimeout(OraclePreparedStatement.java:9892)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10069)\n\t\t... 17 more\n\tCaused by: Error : 12899, Position : 165, Sql = INSERT INTO HOSPITAL_INFO (\"HOS_ID\",\"HOS_NAME\",\"STATUS\",\"HOS_TYPE\",\"MEDI_COURSE\",\"ADDR\",\"POST_CODE\",\"LONGITUDE\",\"LATITUDE\",\"TEL\",\"SIDO_ID\") VALUES (:1 ,:2 ,:3 ,:4 ,:5 ,:6 ,:7 ,:8 ,:9 ,:10 ,:11 ), OriginalSql = INSERT INTO HOSPITAL_INFO (\"HOS_ID\",\"HOS_NAME\",\"STATUS\",\"HOS_TYPE\",\"MEDI_COURSE\",\"ADDR\",\"POST_CODE\",\"LONGITUDE\",\"LATITUDE\",\"TEL\",\"SIDO_ID\") VALUES (?,?,?,?,?,?,?,?,?,?,?), Error Msg = ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"MEDI_COURSE\" (actual: 311, maximum: 200)\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:632)\n\t\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:893)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:69)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:745)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.sql.BatchUpdateException: ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"MEDI_COURSE\" (actual: 311, maximum: 200)\n\n\tat oracle.jdbc.driver.OraclePreparedStatement.generateBatchUpdateException(OraclePreparedStatement.java:10323)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10090)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeLargeBatch(OraclePreparedStatement.java:9975)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatch(OraclePreparedStatement.java:9932)\n\tat oracle.jdbc.driver.OracleStatementWrapper.executeBatch(OracleStatementWrapper.java:262)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:728)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n\tSuppressed: java.sql.SQLException: ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"MEDI_COURSE\" (actual: 311, maximum: 200)\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:628)\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:562)\n\t\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1145)\n\t\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:726)\n\t\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:291)\n\t\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:492)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:148)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1038)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeForRowsWithTimeout(OraclePreparedStatement.java:9892)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10069)\n\t\t... 17 more\n\tCaused by: Error : 12899, Position : 165, Sql = INSERT INTO HOSPITAL_INFO (\"HOS_ID\",\"HOS_NAME\",\"STATUS\",\"HOS_TYPE\",\"MEDI_COURSE\",\"ADDR\",\"POST_CODE\",\"LONGITUDE\",\"LATITUDE\",\"TEL\",\"SIDO_ID\") VALUES (:1 ,:2 ,:3 ,:4 ,:5 ,:6 ,:7 ,:8 ,:9 ,:10 ,:11 ), OriginalSql = INSERT INTO HOSPITAL_INFO (\"HOS_ID\",\"HOS_NAME\",\"STATUS\",\"HOS_TYPE\",\"MEDI_COURSE\",\"ADDR\",\"POST_CODE\",\"LONGITUDE\",\"LATITUDE\",\"TEL\",\"SIDO_ID\") VALUES (?,?,?,?,?,?,?,?,?,?,?), Error Msg = ORA-12899: value too large for column \"DW_HOPITAL\".\"HOSPITAL_INFO\".\"MEDI_COURSE\" (actual: 311, maximum: 200)\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:632)\n\t\t... 26 more\n"}
